# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C2Zj-x1C_W8vQwt1h4_Iunm6rwJDGopt
"""

import pandas as pd
import numpy as np

import seaborn as sns

"""Observe Dataset"""

df = pd.read_csv('/content/sentiment_tweets3.csv')

df.head()

df['label (depression result)'].value_counts()

selected_rows = df[df['label (depression result)'] == 0].head(2314)

selected_rows1= df[df['label (depression result)'] == 1].head(2314)

combined_df = pd.concat([selected_rows1,selected_rows], ignore_index=True)

combined_df['label (depression result)'].value_counts()

combined_df.tail()

combined_df.loc[1031]['message to examine']

combined_df.loc[3]['message to examine']

combined_df.loc[1234]['message to examine']

combined_df.isnull().sum()

# Check if there's any number

for i in combined_df['message to examine']:
  for j in i.split():
    if j.isdigit():
      s = "yes"
    else:
      s = "no"
print(s)

"""**Remove Numbers**"""

import re

# First Remove all the numbers

def remove_numbers(text):
  return ' '.join([i for i in str(text).split() if not i.isdigit()])

combined_df['clean_tweets'] = combined_df['message to examine'].apply(lambda x: remove_numbers(x))

combined_df

# Lowercasing all the tweets

combined_df['clean_tweets'] = combined_df['clean_tweets'].str.lower()

"""**Remove Weblinks**"""

# Removal of Weblinks

def remove_weblinks(text):
  return re.sub(r"http\S+", "", text)

combined_df['clean_tweets2'] = combined_df['clean_tweets'].apply(lambda x: remove_weblinks(x))

combined_df

"""**Remove Twitter Mentions**"""

def remove_twitter(text):
  return re.sub('@[\w]+','',text)

combined_df['clean_tweets3'] = combined_df['clean_tweets2'].apply(lambda x: remove_twitter(x))

combined_df

"""**Removal of Punctuations**"""

import string

PUNCT_TO_REMOVE = string.punctuation

def remove_punctuation(text):
  return text.translate(str.maketrans('','', PUNCT_TO_REMOVE))

combined_df['clean_tweets4'] = combined_df['clean_tweets3'].apply(lambda x: remove_punctuation(x))

combined_df

"""**Removal of Stopwords**"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
", ".join(stopwords.words('english'))

STOPWORDS = set(stopwords.words('english'))

def remove_stopwords(text):
  return " ".join([word for word in str(text).split() if word not in STOPWORDS])

combined_df['clean_tweets5'] = combined_df['clean_tweets4'].apply(lambda x: remove_stopwords(x))

combined_df

"""**Frequent Words**"""

from collections import Counter
cnt = Counter()

for text in combined_df['clean_tweets5'].values:
  for word in text.split():
    cnt[word] += 1

cnt.most_common(10)

"""**Rare words**"""

n_rare_words = 10
RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])

# Let's see what are the Rarewords

RAREWORDS

# Let's remove these

def remove_stopwords(text):
  return " ".join([word for word in str(text).split() if word not in RAREWORDS])

combined_df['clean_tweets6'] = combined_df['clean_tweets5'].apply(lambda x: remove_stopwords(x))

"""**Lemmatization**"""

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

wordnet_map = {"n": wordnet.NOUN, "v": wordnet.VERB, "j": wordnet.ADJ, "r": wordnet.ADV}

def lemmatize_words(text):
    pos_tagged_text = nltk.pos_tag(text.split())
    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.VERB)) for word, pos in pos_tagged_text])

combined_df["text_lemmatized"] = combined_df['clean_tweets6'].apply(lambda text: lemmatize_words(text))

combined_df

"""**Replace some short words**"""

short_words = {
"aint": "am not",
"arent": "are not",
"cant": "cannot",
"'cause": "because",
"couldve": "could have",
"couldnt": "could not",
"didnt": "did not",
"doesnt": "does not",
"dont": "do not",
"hadnt": "had not",
"hasnt": "has not",
"havent": "have not",
"im": "I am",
"em": "them",
"ive": "I have",
"isnt": "is not",
"lets": "let us",
"theyre": "they are",
"theyve": "they have",
"wasnt": "was not",
"well": "we will",
"were": "we are",
"werent": "were not",
"you're": "you are",
"you've": "you have"
}

def replace_short_words(text):
  for word in text.split():
    if word in short_words:
      text = text.replace(word, short_words[word])

  return text

combined_df["clean_tweets7"] = combined_df['text_lemmatized'].apply(lambda text: replace_short_words(text))

combined_df

"""**Our text is clean and ready for training. But let's delete all the previous columns.**"""

xdf = combined_df[['Index','label (depression result)','clean_tweets7']]

xdf

xdf.columns = ['Index','Labels','Tweets']

xdf

# Plot the word cloud

from wordcloud import WordCloud

import matplotlib.pyplot as plt

sentences = xdf['Tweets'].tolist()

len(sentences)

# Joining sentences (combining all the sentences that we have)

joined_sentences = " ".join(sentences)

plt.figure(figsize = (12,8))
plt.imshow(WordCloud().generate(joined_sentences));

xdf['Labels'].value_counts()

"""# Let's visualize postive and negative tweets"""

positive_tweets = xdf[xdf['Labels'] == 0]
positive_sentences = positive_tweets['Tweets'].tolist()
positive_string = " ".join(positive_sentences)

plt.figure(figsize = (12,8))
plt.imshow(WordCloud().generate(positive_string));

"""# Let's visualize negative tweets

**bold text**

"""

negative_tweets = xdf[xdf['Labels'] == 1]
negative_sentences = negative_tweets['Tweets'].tolist()
negative_string = " ".join(negative_sentences)

plt.figure(figsize = (12,8))
plt.imshow(WordCloud().generate(negative_string));

"""**Basic Sentiment Analaysis**
First, we will build our model with Basic Sentiment Analysis technique with tf-idf and NaiveBayes Classifier
"""

from sklearn.feature_extraction.text import TfidfVectorizer

cv = TfidfVectorizer()

tfidf = cv.fit_transform(xdf['Tweets'])

"""**Splitting Dataset**"""

from sklearn.model_selection import train_test_split

tfX_train, tfX_test, tfy_train, tfy_test = train_test_split(tfidf, xdf['Labels'], test_size = 0.2)

tfX_train

tfX_train.shape

"""**Models and Evaluation**"""

## NaiveBayes

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB()

mnb.fit(tfX_train, tfy_train)

from sklearn.metrics import confusion_matrix, accuracy_score

y_pred_mnb = mnb.predict(tfX_test)

print(f'Accuracy score is : {accuracy_score(tfy_test, y_pred_mnb)}')

cf = confusion_matrix(tfy_test, y_pred_mnb, labels = [1,0])
cf

x_axis_labels = ["Positive(1)","Negative(0)"]
y_axis_labels = ["Positive(1)","Negative(0)"]

plt.figure(figsize = (8,6))
sns.set(font_scale=1)
sns.heatmap(cf, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, fmt='g',annot_kws = {'size': 16})
plt.xlabel("Actual Class", fontsize = 20)
plt.ylabel("Predicted Class", fontsize = 20)
plt.show()

"""**Better Sentiment Analaysis**"""

import tensorflow as tf
from tensorflow.keras.layers import Dense, Embedding,LSTM, SimpleRNN ,Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
import tensorflow_hub as hub

# Load Pretrained Word2Vec

embed = hub.load("https://tfhub.dev/google/Wiki-words-250/2")

def get_max_length(df):
  ## get max token counts from train data,
  ## so we use this number as fixed length input to RNN cell

  max_length = 0
  for row in xdf['Tweets']:
    if len(row.split(" ")) > max_length:
      max_length = len(row.split(" "))

  return max_length

get_max_length(xdf['Tweets'])

def get_word2vec_enc(tweets):
  ## get word2vec value for each word in sentence
  # concatenate word in numpy array, so we can use it as RNN input

  encoded_tweets = []
  for tweet in tweets:
    tokens = tweet.split(" ")
    word2vec_embedding = embed(tokens)
    encoded_tweets.append(word2vec_embedding)
  return encoded_tweets

def get_padded_encoded_tweets(encoded_tweets):
  # for short sentences, we prepend zero padding so all input to RNN
  # has same length

  padded_tweets_encoding = []
  for enc_tweet in encoded_tweets:
    zero_padding_cnt = max_length - enc_tweet.shape[0]
    pad = np.zeros((1, 250))
    for i in range(zero_padding_cnt):
      enc_tweet = np.concatenate((pad, enc_tweet), axis = 0)
    padded_tweets_encoding.append(enc_tweet)
  return padded_tweets_encoding

def sentiment_encode(sentiment):
    if sentiment == 0:
        return [0,1]
    else:
        return [1,0]

def preprocess(df):
  # encode text value to numeric value

  tweets = df['Tweets'].tolist()

  encoded_tweets = get_word2vec_enc(tweets)
  padded_encoded_tweets = get_padded_encoded_tweets(encoded_tweets)

  #encoded sentiment
  sentiments = df['Labels'].tolist()
  encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]

  X = np.array(padded_encoded_tweets)
  Y = np.array(encoded_sentiment)

  return X, Y

# Preprocess

max_length = get_max_length(xdf)
max_length

tdf = xdf.sample(frac = 1)
train = tdf[:3702]
test = tdf[3702:]

train.shape, test.shape

train_X, train_Y = preprocess(train)
test_X, test_Y = preprocess(test)

## Build Model

# LSTM

model = Sequential()
model.add(Bidirectional(LSTM(32)))
model.add(Dense(2, activation = 'softmax'))

model.compile(loss = 'categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

"""**Train**"""

model.fit(train_X, train_Y, epochs = 10)

model.summary()

"""**Test**"""

score, acc = model.evaluate(test_X, test_Y, verbose = 2)
print("Test Score:", score)
print("Test Accuracy:", acc)

# Confusion Matrix

y_pred = model.predict(test_X)

import sklearn.metrics as metrics

matrix = metrics.confusion_matrix(test_Y.argmax(axis = 1), y_pred.argmax(axis = 1), labels = [1,0])

matrix

x_axis_labels = ["Positive(1)","Negative(0)"]
y_axis_labels = ["Positive(1)","Negative(0)"]

plt.figure(figsize = (8,6))
sns.set(font_scale=1)
sns.heatmap(matrix, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, fmt='g',annot_kws = {'size': 16})
plt.xlabel("Actual Class", fontsize = 20)
plt.ylabel("Predicted Class", fontsize = 20)
plt.show()

# Save Model
with open('model1.pkl','wb') as f:
    pickle.dump(model,f)

# Load model
with open('model1.pkl', 'rb') as f:
    model = pickle.load(f)